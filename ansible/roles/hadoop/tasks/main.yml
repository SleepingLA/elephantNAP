---

- include_vars: "{{ nodesfile }}"

- group: name={{ cluster_group}} state=present
- user: name={{ cluster_user }} comment="Hadoop" group={{ cluster_group}} shell=/bin/bash createhome=yes skeleton=/etc/skel
#- vars:
- set_fact: hadoop_v="2.7.4" spark_v="2.2.0-bin-hadoop2.7"
#- set_fact: hadoop_v="2.7.3" spark_v="2.0.2-bin-hadoop2.7"

#
- name: unpack hadoop
  unarchive: src=./roles/hadoop/files/hadoop-{{ hadoop_v }}.tar.gz dest=/usr/local owner={{ cluster_user}} group={{ cluster_group }} creates=/usr/local/hadoop

- command: mv /usr/local/hadoop-{{ hadoop_v }} /usr/local/hadoop creates=/usr/local/hadoop removes=/usr/local/hadoop-{{ hadoop_v }}

- name: Ajout du bash_profile
  template: src=.bash_profile dest=/home/{{ cluster_user }}/.bash_profile owner={{ cluster_user }} group={{ cluster_group}}

- lineinfile: dest=/home/{{ cluster_user }}/.bash_profile regexp="HADOOP_HOME=" line="export HADOOP_HOME=/usr/local/hadoop"
- lineinfile: dest=/home/{{ cluster_user }}/.bash_profile regexp="HADOOP_CONF_DIR=" line="export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop"
#- lineinfile: dest=/home/{{ hadoop_user }}/.bashrc regexp="PATH=" line="export PATH=$PATH:$HADOOP_HOME/bin"
- lineinfile: dest=/home/{{ cluster_user }}/.bash_profile regexp="SPARK_HOME=" line="export SPARK_HOME=/usr/local/spark"
- lineinfile: dest=/home/{{ cluster_user }}/.bash_profile regexp="export PATH=\$HADOOP" line="export PATH=$HADOOP_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH" state=present
#- lineinfile: dest=/home/{{ hadoop_user }}/.bashrc regexp="PATH=" line="export PATH=$PATH:$HADOOP_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin"

- name: unpack spark
  unarchive: src=./roles/hadoop/files/spark-{{ spark_v }}.tgz dest=/usr/local owner={{ cluster_user}} group={{ cluster_group }} creates=/usr/local/spark
- command: mv /usr/local/spark-{{ spark_v }} /usr/local/spark creates=/usr/local/spark removes=/usr/local/spark-{{ spark_v }}
#- name: ssh
#  StrictHostKeyChecking no

# Idempotent way to build a /etc/hosts file with Ansible using your Ansible hosts inventory for a source.
# Will include all hosts the playbook is run on.
# Inspired from http://xmeblog.blogspot.com/2013/06/ansible-dynamicaly-update-etchosts.html

- name: "Build hosts file"
  lineinfile: dest=/etc/hosts regexp='{{ item.hostname }}' line="{{ item.ip }} {{ item.hostname }}" state=present
  with_items: "{{ nodes }}"
  #lineinfile: dest=/etc/hosts regexp='{{ item.ip }}' line="{{ item.ip }} {{ item.hostname }}" state=present # apres un redeploy ladrrese change pas le nom

- lineinfile: dest=/etc/hosts regexp='127.0.1.1' state=absent

- file: path=/home/{{ cluster_user }}/tmp state=directory owner={{ cluster_user}} group={{ cluster_group }} mode=750
- file: path=/home/{{ cluster_user }}/hadoop-data/hdfs/namenode state=directory owner={{ cluster_user}} group={{ cluster_group }} mode=750
- file: path=/home/{{ cluster_user }}/hadoop-data/hdfs/datanode state=directory owner={{ cluster_user}} group={{ cluster_group }} mode=750

- name: Add the service scripts
  template: src={{ item.src }} dest={{ item.dest }} owner={{ cluster_user}} group={{ cluster_group }}
  with_items:
    - {src: "core-site.xml", dest: "/usr/local/hadoop/etc/hadoop/core-site.xml"}
    - {src: "hdfs-site.xml", dest: "/usr/local/hadoop/etc/hadoop/hdfs-site.xml"}
    - {src: "yarn-site.xml", dest: "/usr/local/hadoop/etc/hadoop/yarn-site.xml"}
    - {src: "mapred-site.xml", dest: "/usr/local/hadoop/etc/hadoop/mapred-site.xml"}
    - {src: "spark-defaults.conf", dest: "/usr/local/spark/conf/spark-defaults.conf"}
    - {src: "core-site.xml", dest: "/usr/local/spark/conf/core-site.xml"}

#- lineinfile: dest=/usr/local/hadoop/etc/hadoop/hadoop-env.sh regexp="^export JAVA_HOME" line="export JAVA_HOME=/opt/jdk1.8.0_66/"
- name: set  JAVA HOME
  when: is_ubuntu
  lineinfile: dest=/usr/local/hadoop/etc/hadoop/hadoop-env.sh regexp="^export JAVA_HOME" line="export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64"

- name: set  JAVA HOME
  when: is_centos
  lineinfile: dest=/usr/local/hadoop/etc/hadoop/hadoop-env.sh regexp="^export JAVA_HOME" line="export JAVA_HOME=/usr/lib/jvm/jre"

#- lineinfile: dest=/usr/local/hadoop/etc/hadoop/hadoop-env.sh regexp="^export JAVA_HOME" line="export JAVA_HOME=/usr/lib/jvm/jre"
#- lineinfile: dest=/usr/local/hadoop/etc/hadoop/hadoop-env.sh regexp="^export JAVA_HOME" line="export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64"

# spark env

# verifier si le mettre dans .bashrc est ok
#- name: copy spark-env dans /etc/profile.d
#  template: src=spark.sh dest=/etc/profile.d/spark.sh


# meme plus sur que ca soit requis a verifier
- command: cp  /usr/local/hadoop/share/hadoop/tools/lib/hadoop-openstack-{{ hadoop_v }}.jar /usr/local/hadoop/share/hadoop/common/lib

# pour spark cest requis sinon on doit le mentionner a la ligne de commande
- command: cp  /usr/local/hadoop/share/hadoop/tools/lib/hadoop-openstack-{{ hadoop_v }}.jar /usr/local/spark/jars
#- command: cp  /usr/local/hadoop/share/hadoop/tools/lib/aws-java-sdk-1.7.4.jar  /usr/local/hadoop/share/hadoop/tools/lib/hadoop-aws--{{ hadoop_v }}.jar /usr/local/hadoop/share/hadoop/common/lib
- shell: cp  /usr/local/hadoop/share/hadoop/tools/lib/aws-java-sdk-*.jar  /usr/local/hadoop/share/hadoop/tools/lib/hadoop-aws-*.jar /usr/local/hadoop/share/hadoop/common/lib

